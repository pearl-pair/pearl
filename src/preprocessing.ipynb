{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9f89ab-3bab-4af6-a5a5-f529ea5f9819",
   "metadata": {},
   "source": [
    "# TRAIN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31253c8b-2153-4326-80e1-e6e5aa833248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import datasets\n",
    "# datasets.disable_caching()\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def mask_text(sentences: list, verbalize: bool = False):\n",
    "    masked_texts = []\n",
    "    for sentence in sentences:\n",
    "        orig_text = sentence.to_original_text()\n",
    "        new_text = \"\"\n",
    "        prev_e_idx = 0\n",
    "        for ent in sentence.to_dict()['entities']:\n",
    "            s_idx, e_idx = ent['start_pos'], ent['end_pos']\n",
    "            cur_entity = ent['labels'][0]['value']\n",
    "            \n",
    "            if verbalize:\n",
    "                verbalized_entity = None\n",
    "                if cur_entity == \"LOC\":\n",
    "                    verbalized_entity = \"is location\"\n",
    "                elif cur_entity == \"TIM\":\n",
    "                    verbalized_entity = \"is time\"\n",
    "                elif cur_entity == \"PER\":\n",
    "                    verbalized_entity = \"is person\"\n",
    "                elif cur_entity == \"EVT\":\n",
    "                    verbalized_entity = \"is event\"                \n",
    "                # elif cur_entity == \"ORG\":\n",
    "                #     verbalized_entity = \"is organization\"\n",
    "                # elif cur_entity == \"DAT\":\n",
    "                #     verbalized_entity = \"is date\"\n",
    "                new_text += orig_text[prev_e_idx:s_idx] + f\"[{orig_text[s_idx:e_idx]} {verbalized_entity}]\"\n",
    "                \n",
    "            else:\n",
    "                # new_text += orig_text[prev_e_idx:s_idx] + f\"[{orig_text[s_idx:e_idx]}]=<{ent['labels'][0]['value']}>\"\n",
    "                new_text += orig_text[prev_e_idx:s_idx] + f\"<{cur_entity}> {orig_text[s_idx:e_idx]} </{cur_entity}>\"\n",
    "            prev_e_idx = e_idx\n",
    "        \n",
    "        new_text += orig_text[prev_e_idx:]\n",
    "        \n",
    "        masked_texts.append(new_text)\n",
    "    return masked_texts\n",
    "\n",
    "def read_id_text_from_file(file_name, is_json = False, has_tp = False):\n",
    "    ids_, texts_, tps_ = [], [], []\n",
    "    if is_json:\n",
    "        data = datasets.load_dataset(\n",
    "            'json',\n",
    "            data_files=file_name,\n",
    "        )['train']\n",
    "        ids_, texts_ = data['text_id'], data['text']\n",
    "        if has_tp:\n",
    "            tps_ = data['is_tp']\n",
    "    else:  # for .tsv \n",
    "        with open(file_name, \"r\") as f:\n",
    "            for data in f:\n",
    "                id_, text_ = data.split('\\t')\n",
    "                ids_.append(id_)\n",
    "                texts_.append(text_)\n",
    "    return ids_, texts_, tps_\n",
    "\n",
    "def passage_dict_to_text(psg_dict: dict, simplify: bool = False):\n",
    "    psg = \"\"\n",
    "    all_key_values = []\n",
    "    for k_, v_ in psg_dict.items():\n",
    "        if k_ == 'ct':\n",
    "            pass\n",
    "        elif k_ == 'date':\n",
    "            if simplify:\n",
    "                pass\n",
    "            else:\n",
    "                all_key_values.append(f\"{k_}: date\")\n",
    "        elif k_ == 'attendees':\n",
    "            if simplify:\n",
    "                all_key_values.append(f\"{' , '.join(v_)}\")\n",
    "            else:\n",
    "                all_key_values.append(f\"{k_}: {' , '.join(v_)}\")\n",
    "        else:\n",
    "            if simplify:\n",
    "                all_key_values.append(f\"{v_}\")\n",
    "            else:\n",
    "                all_key_values.append(f\"{k_}: {v_}\")\n",
    "    return ' | '.join(all_key_values)\n",
    "\n",
    "def parse_json(json_):\n",
    "    all_dict = defaultdict(list)\n",
    "    idx = 0\n",
    "    for _ in json_:\n",
    "        if 'ct' not in _['passage'] or 'ct' not in _['query']:\n",
    "            continue\n",
    "        text_ = passage_dict_to_text(_['passage'], simplify=True)\n",
    "        all_dict['docid'].append(idx)\n",
    "        all_dict['text'].append(text_)\n",
    "        all_dict['orig_text'].append(text_)\n",
    "        all_dict['d2q_text'].append(passage_dict_to_text(_['passage'], simplify=False))\n",
    "        all_dict['text_id'].append(_['query']['txt']) # query\n",
    "        all_dict['q_ct'].append(_['query']['ct'])\n",
    "        if 'title' in _['passage']: # if calendar data, then creation time (for filter) is date instead\n",
    "            all_dict['p_ct'].append(_['passage']['date'])\n",
    "        else:\n",
    "            all_dict['p_ct'].append(_['passage']['ct'])\n",
    "\n",
    "        if 'tagged_query' in _ and 'tagged_passage' in _:\n",
    "            all_dict['tagged_query'].append(_['tagged_query'])\n",
    "            all_dict['tagged_passage'].append(_['tagged_passage'])\n",
    "        else:\n",
    "            all_dict['tagged_query'].append(None)\n",
    "            all_dict['tagged_passage'].append(None)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "    return all_dict \n",
    "\n",
    "def load_cluster_data(vector_tsv_path, mapping_pkl_path, v_dim=768):\n",
    "    \"\"\"\n",
    "    Load clustered embeddings and ID mapping.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(vector_tsv_path, header=None, sep='\\t',\n",
    "                     names=['docid', 'url', 'title', 'body', 'anchor', 'click', 'language', 'vector']).loc[:, ['docid', 'vector']]\n",
    "    df.drop_duplicates('docid', inplace=True)\n",
    "\n",
    "    # Parse vectors\n",
    "    doc_ids = df['docid'].tolist()\n",
    "    vectors = df['vector'].apply(lambda v: [float(x) for x in v.split('|')])\n",
    "    X = np.array(vectors.tolist())\n",
    "\n",
    "    # Load hierarchical cluster ID mapping\n",
    "    with open(mapping_pkl_path, 'rb') as f:\n",
    "        id_mapping = pickle.load(f)\n",
    "\n",
    "    return doc_ids, X, id_mapping\n",
    "\n",
    "def assign_cluster_id(new_embedding, clustered_embeddings, id_mapping, doc_ids, used_ids):\n",
    "    \"\"\"\n",
    "    Given a new embedding, return the cluster ID of the closest existing embedding.\n",
    "    \"\"\"\n",
    "    distances = cosine_distances([new_embedding], clustered_embeddings)[0]\n",
    "    for close_idx in np.argsort(distances):  \n",
    "        assert doc_ids[close_idx] == close_idx, \"doc_ids[close_idx] should be equivalent to close_idx\"\n",
    "        close_docid_ = id_mapping[doc_ids[close_idx]] # e.g., ['1', '7', '4', '12']\n",
    "        str_close_docid_ = ''.join([str(_) for _ in close_docid_])\n",
    "        if str_close_docid_ in used_ids:\n",
    "            pass\n",
    "        else:\n",
    "            return id_mapping[doc_ids[close_idx]], close_docid_\n",
    "\n",
    "def get_exisiting_cluster_ids(test_masked_texts, use_annotation=True):\n",
    "    path_prefix = dataset + \"_seen\" + \"_masked\" if use_annotation else dataset + \"_seen\"\n",
    "    doc_ids, clustered_X, id_mapping = load_cluster_data(\n",
    "        vector_tsv_path=f'data/out/{path_prefix}_doc_content_embedding_bert_512.tsv',\n",
    "        mapping_pkl_path=f'IDMapping_{path_prefix}_bert_512_k9_c20_seed_7.pkl',\n",
    "        v_dim=768  \n",
    "    )\n",
    "    \"\"\"\n",
    "    doc_ids = docids of the training samples (not clustered ones, just 0,1,2,... \"_masked\" denotes masked sentences' embeddings\n",
    "    id_mapping = 0,1,2,3,.. -> clustered docid {0: ['1', '7', '4', '12'], ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\").to(f'cuda:0')\n",
    "    \n",
    "    test_texts_chunk = [test_masked_texts[i*20:(i+1)*20] for i in range((len(test_masked_texts) // 20) + 1)]\n",
    "    \n",
    "    import numpy as np\n",
    "    output = []\n",
    "    for test_texts_ in test_texts_chunk:\n",
    "        if test_texts_ != []:\n",
    "            encoded_input = tokenizer(test_texts_, max_length=512, return_tensors='pt', padding=True, truncation=True).to(f'cuda:0')\n",
    "            output.extend(model(**encoded_input, return_dict=True).last_hidden_state.detach().cpu()[:, 0, :].numpy().tolist())\n",
    "    \n",
    "    test_texts_docids = []\n",
    "    test_char_cluster_ids = []\n",
    "    from tqdm import tqdm\n",
    "    for test_text_emb in tqdm(output):\n",
    "        cluster_id, char_cluster_ids = assign_cluster_id(test_text_emb, clustered_X, id_mapping, doc_ids, test_texts_docids) # '6153', [6, 1, 5, 3] \n",
    "        test_char_cluster_ids.append(' '.join([str(_) for _ in char_cluster_ids]))\n",
    "        test_texts_docids.append(''.join([str(_) for _ in cluster_id]))\n",
    "    return test_texts_docids, test_char_cluster_ids\n",
    "\n",
    "import re\n",
    "import calendar\n",
    "import dateparser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_relative_time(query, creation_time):\n",
    "    settings = {'RELATIVE_BASE': creation_time}\n",
    "    q = re.sub(r'[^A-Za-z0-9\\s\\-\\:]', '', query.strip())\n",
    "    query_lower = q.lower()\n",
    "\n",
    "    weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    months = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "    seasons = {\n",
    "        \"spring\": (3, 5),\n",
    "        \"summer\": (6, 8),\n",
    "        \"fall\": (9, 11),\n",
    "        \"autumn\": (9, 11),\n",
    "        \"winter\": (12, 2)  # spans year-end\n",
    "    }\n",
    "\n",
    "    # --- helper funcs -----------------------------------------------------\n",
    "    def month_range(y: int, m: int):\n",
    "        last_day = calendar.monthrange(y, m)[1]\n",
    "        return datetime(y, m, 1, 0, 0, 0), datetime(y, m, last_day, 23, 59, 59)\n",
    "\n",
    "    def weekday_last(target_idx: int):\n",
    "        days_back = (creation_time.weekday() - target_idx + 7) % 7 or 7\n",
    "        day = creation_time - timedelta(days=days_back)\n",
    "        return day.replace(hour=0, minute=0, second=0, microsecond=0), day.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1) Explicit month modifiers  (\"last June\", \"next March\", \"June\")\n",
    "    m_mod_match = re.fullmatch(r'(?:\\b(last|next|this)\\s+)?(' + '|'.join(months) + r')\\b', query_lower)\n",
    "    if m_mod_match:\n",
    "        mod, month_word = m_mod_match.groups()\n",
    "        month_idx = months.index(month_word) + 1\n",
    "        year = creation_time.year\n",
    "        if mod == 'last':\n",
    "            year -= 1\n",
    "        elif mod == 'next':\n",
    "            year += 1\n",
    "        start_time, end_time = month_range(year, month_idx)\n",
    "        return start_time, end_time\n",
    "\n",
    "    # 2) \"beginning of <month>\"  -> first half (day 1-15)\n",
    "    beg_match = re.fullmatch(r'beginning of (' + '|'.join(months) + r')', query_lower)\n",
    "    if beg_match:\n",
    "        month_word = beg_match.group(1)\n",
    "        month_idx = months.index(month_word) + 1\n",
    "        year = creation_time.year\n",
    "        start_time = datetime(year, month_idx, 1, 0, 0, 0)\n",
    "        end_time = datetime(year, month_idx, 15, 23, 59, 59)\n",
    "        return start_time, end_time\n",
    "\n",
    "    # 3) \"last <weekday>\"  (entire previous week's weekday)\n",
    "    for idx, wd in enumerate(weekdays):\n",
    "        if query_lower == f'last {wd}':\n",
    "            return weekday_last(idx)\n",
    "\n",
    "    # ---- existing handler chain -----------------------------------------\n",
    "\n",
    "    # Special vague patterns\n",
    "    vague_period_match = re.search(r'(couple of|few)\\s+(weekends?|weeks?|days?|months?|years?)\\s+ago', query_lower)\n",
    "\n",
    "    numeric_period_match = re.search(r'(\\d+)\\s+(day|week|month|year)s?\\s+ago', query_lower)\n",
    "    text_period_match = re.search(r'(one|two|three|four|five|six|seven|eight|nine|ten)\\s+(day|week|month|year)s?\\s+ago', query_lower)\n",
    "    text2num = {\n",
    "        \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "        \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "        \"couple of\": 2, \"few\": 3\n",
    "    }\n",
    "\n",
    "    start_time = end_time = None\n",
    "\n",
    "    # Seasonal terms (this/last/next spring etc.)\n",
    "    for season, (sm, em) in seasons.items():\n",
    "        if f'this {season}' in query_lower:\n",
    "            year = creation_time.year\n",
    "        elif f'last {season}' in query_lower:\n",
    "            year = creation_time.year - 1\n",
    "        elif f'next {season}' in query_lower:\n",
    "            year = creation_time.year + 1\n",
    "        else:\n",
    "            continue\n",
    "        if season == 'winter':\n",
    "            st_year = year if sm <= em else year - 1\n",
    "            start_time = datetime(st_year, sm, 1, 0, 0, 0)\n",
    "            end_year = st_year if em >= sm else st_year + 1\n",
    "            end_time = datetime(end_year, em, calendar.monthrange(end_year, em)[1], 23, 59, 59)\n",
    "        else:\n",
    "            start_time = datetime(year, sm, 1, 0, 0, 0)\n",
    "            end_time = datetime(year, em, calendar.monthrange(year, em)[1], 23, 59, 59)\n",
    "        return start_time, end_time\n",
    "\n",
    "    # quick explicit cases\n",
    "    if 'tonight' in query_lower:\n",
    "        start_time = creation_time.replace(hour=18, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time.replace(hour=23, minute=59, second=59)\n",
    "    elif 'upcoming' in query_lower:\n",
    "        start_time = creation_time\n",
    "        end_time = creation_time + timedelta(days=30)\n",
    "    elif vague_period_match:\n",
    "        quant, unit = vague_period_match.groups()\n",
    "        num = text2num[quant]\n",
    "        if 'weekend' in unit or 'week' in unit:\n",
    "            delta_days = 7 * num\n",
    "        elif 'day' in unit:\n",
    "            delta_days = num\n",
    "        elif 'month' in unit:\n",
    "            delta_days = 30 * num\n",
    "        else:\n",
    "            delta_days = 365 * num\n",
    "        start_time = (creation_time - timedelta(days=delta_days)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time\n",
    "    elif any(t in query_lower for t in ['yesterday', 'last night']):\n",
    "        start_time = (creation_time - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = start_time.replace(hour=23, minute=59, second=59)\n",
    "    elif 'tomorrow' in query_lower:\n",
    "        start_time = (creation_time + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = start_time.replace(hour=23, minute=59, second=59)\n",
    "    elif 'today' in query_lower or 'this day' in query_lower:\n",
    "        start_time = creation_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time.replace(hour=23, minute=59, second=59)\n",
    "    elif any(kw in query_lower for kw in ['last week', 'next week', 'this week']):\n",
    "        wd = creation_time.weekday()\n",
    "        if 'last week' in query_lower:\n",
    "            start_time = (creation_time - timedelta(days=wd + 7)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        elif 'next week' in query_lower:\n",
    "            start_time = (creation_time + timedelta(days=7 - wd)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        else:\n",
    "            start_time = (creation_time - timedelta(days=wd)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = start_time + timedelta(days=6, hours=23, minutes=59, seconds=59)\n",
    "    elif any(kw in query_lower for kw in ['last month', 'next month', 'this month']):\n",
    "        first_this = creation_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "        if 'last month' in query_lower:\n",
    "            last_end = first_this - timedelta(seconds=1)\n",
    "            start_time = last_end.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "            end_time = last_end\n",
    "        elif 'next month' in query_lower:\n",
    "            next_first = (first_this + timedelta(days=32)).replace(day=1)\n",
    "            start_time = next_first\n",
    "            end_time = (next_first + timedelta(days=32)).replace(day=1) - timedelta(seconds=1)\n",
    "        else:\n",
    "            start_time = first_this\n",
    "            end_time = (first_this + timedelta(days=32)).replace(day=1) - timedelta(seconds=1)\n",
    "    elif any(kw in query_lower for kw in ['last year', 'next year', 'this year']):\n",
    "        if 'last year' in query_lower:\n",
    "            yr = creation_time.year - 1\n",
    "        elif 'next year' in query_lower:\n",
    "            yr = creation_time.year + 1\n",
    "        else:\n",
    "            yr = creation_time.year\n",
    "        start_time = datetime(yr, 1, 1, 0, 0, 0)\n",
    "        end_time = datetime(yr, 12, 31, 23, 59, 59)\n",
    "    elif any(kw in query_lower for kw in ['last weekend', 'next weekend', 'this weekend']):\n",
    "        saturday = creation_time - timedelta(days=creation_time.weekday() - 5)\n",
    "        if 'last weekend' in query_lower:\n",
    "            saturday -= timedelta(days=7)\n",
    "        elif 'next weekend' in query_lower:\n",
    "            saturday += timedelta(days=7)\n",
    "        start_time = saturday.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = start_time + timedelta(days=1, hours=23, minutes=59, seconds=59)\n",
    "    elif 'afternoon' in query_lower:\n",
    "        start_time = creation_time.replace(hour=12, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time.replace(hour=23, minute=59, second=59)\n",
    "    elif 'morning' in query_lower:\n",
    "        start_time = creation_time.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time.replace(hour=11, minute=59, second=59)\n",
    "    elif numeric_period_match:\n",
    "        n, unit = int(numeric_period_match.group(1)), numeric_period_match.group(2)\n",
    "        delta = {'day':1, 'week':7, 'month':30, 'year':365}[unit] * n\n",
    "        start_time = (creation_time - timedelta(days=delta)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time\n",
    "    elif text_period_match:\n",
    "        word, unit = text_period_match.group(1), text_period_match.group(2)\n",
    "        n = text2num[word]\n",
    "        delta = {'day':1, 'week':7, 'month':30, 'year':365}[unit] * n\n",
    "        start_time = (creation_time - timedelta(days=delta)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = creation_time\n",
    "    elif 'last' in query_lower and not any(k in query_lower for k in weekdays + months):\n",
    "        start_time = creation_time - timedelta(days=365)\n",
    "        end_time = creation_time\n",
    "    else:\n",
    "        parsed = dateparser.parse(q, settings=settings)\n",
    "        if not parsed:\n",
    "            return None\n",
    "        start_time = parsed.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        end_time = parsed.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def extract_dat_from_sent(sent, ct):\n",
    "    \"\"\"\n",
    "    returns start_time and end_time in isoformat if sent contains `any` `DAT` entity within the original text\n",
    "        (1st rule) Use DAT\n",
    "        (2nd rule) Use TIM \n",
    "    returns (\"-\", \"-\") otherwise\n",
    "\n",
    "    TODO:: instead of parse_relative_time for every entity, save `valid` entity to reduce time complexity significantly\n",
    "        --> pre-parse, collect `valid` entity, then check if it is in (?)\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    \n",
    "    # (1st rule) Check for the DAT \n",
    "    for ent in sent.to_dict()['entities']:\n",
    "        ent_text, ent_type = ent['text'], ent['labels'][0]['value']\n",
    "        if ent_type == 'TIM':\n",
    "            result = parse_relative_time(ent_text, ct)\n",
    "            if result:  # If at least one valid `DAT` entity is found, stop here\n",
    "                break\n",
    "    # # (2nd rule) Check for the TIM\n",
    "    # if result is None:\n",
    "    #     for ent in sent.to_dict()['entities']:\n",
    "    #         ent_text, ent_type = ent['text'], ent['labels'][0]['value']\n",
    "    #         if ent_type == 'TIM':\n",
    "    #             result = parse_relative_time(ent_text, ct)\n",
    "    #             if result:\n",
    "    #                 break\n",
    "\n",
    "    if result:\n",
    "        return result[0].isoformat(), result[1].isoformat()\n",
    "    else:\n",
    "        return (\"-\", \"-\")\n",
    "\n",
    "dataset = \"syn_50k\" # syn_8k, all_gen_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abe3e5-a494-430a-b424-3232a500538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_path = f\"data/synthetics/{dataset}.tsv\"\n",
    "fine_tuned_tagger = SequenceTagger.load('./fine-tuned-model/final-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0583c-8179-4080-bccf-1a7e61f33ac9",
   "metadata": {},
   "source": [
    "## Load and Parse Raw-data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21102ed-d5de-41f4-a6f8-72e5a72374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/synthetics/{dataset}.json\") as f:\n",
    "    json_ = json.load(f)\n",
    "\n",
    "print(f\"num total samples: {len(json_)}\")\n",
    "all_dict = parse_json(json_)\n",
    "print(f\"num total after anomaly discards: {len(all_dict['docid'])}\")\n",
    "print(f\"num NER samples: {sum([1 if _ is not None else 0 for _ in all_dict['tagged_query']])}\")\n",
    "\n",
    "print('--------------------------------')\n",
    "print(f\"id: {all_dict['docid'][-1]}\")\n",
    "print(all_dict['text'][-1])\n",
    "print(all_dict['d2q_text'][-1])\n",
    "print(all_dict['text_id'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c6a20-0e23-4a4b-bea2-b8c974911194",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NER Dataset Generation (optional) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0219156-a0fd-4002-ab6c-36ff6fe02c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_entity_pairs(text: str, entities: list):\n",
    "    entity_text = text\n",
    "    if len(entities) > 0:\n",
    "        for ent in entities:\n",
    "            k_, v_ = next(iter(ent.items()))  \n",
    "            \n",
    "            # If entity exists, find and replace with entity\n",
    "            # print(v_)\n",
    "            if v_ in entity_text:\n",
    "                # Get BIO entity for v_\n",
    "                v_bio_ = []\n",
    "                for idx_, split_ in enumerate(v_.split(\" \")):  \n",
    "                    if idx_ == 0:\n",
    "                        v_bio_.append(f\"B-{k_}\")\n",
    "                    else:\n",
    "                        v_bio_.append(f\"I-{k_}\")\n",
    "                v_bio_text = \" \".join(v_bio_)\n",
    "\n",
    "                ent_s_idx = entity_text.find(v_)\n",
    "                ent_e_idx = ent_s_idx + len(v_)\n",
    "                entity_text = entity_text[:ent_s_idx] + v_bio_text + entity_text[ent_e_idx:]\n",
    "\n",
    "    word_entity_pairs = []\n",
    "    for word, ent in zip(text.split(\" \"), entity_text.split(\" \")):\n",
    "        if \"B-\" in ent or \"I-\" in ent:\n",
    "            trunc_ent = ent[:5] \n",
    "            if not trunc_ent[2:] in [\"LOC\", \"PER\", \"EVT\", \"TIM\"]: # [\"LOC\", \"PER\", \"DAT\", \"ORG\", \"TIM\"]:\n",
    "                print(f\"Bad entity name detected: {trunc_ent[2:]} for the word: {word}\")\n",
    "            else:\n",
    "                word_entity_pairs.append((word, ent[:5]))\n",
    "        else:\n",
    "            word_entity_pairs.append((word, \"O\"))\n",
    "    return word_entity_pairs\n",
    "\n",
    "token_entity_pairs_lst = []\n",
    "for idx_ in range(len(all_dict['docid'])):\n",
    "    if all_dict['tagged_query'][idx_] is not None:\n",
    "        q, p, t_q, t_p = all_dict['text_id'][idx_], all_dict['text'][idx_], all_dict['tagged_query'][idx_], all_dict['tagged_passage'][idx_]\n",
    "        token_entity_pairs_lst.append(get_word_entity_pairs(text=q, entities=t_q)) \n",
    "        token_entity_pairs_lst.append(get_word_entity_pairs(text=p, entities=t_p))\n",
    "        \n",
    "ner_save_path = \"data/synthetics/ner\"\n",
    "split_ratio = \"7:1:2\"\n",
    "tot_pairs = len(token_entity_pairs_lst)\n",
    "trn_idx = int(tot_pairs * int(split_ratio.split(\":\")[0]) / 10)\n",
    "dev_idx = int(trn_idx + tot_pairs * int(split_ratio.split(\":\")[1]) / 10)\n",
    "\n",
    "all_w_str = []\n",
    "for pairs in token_entity_pairs_lst:\n",
    "    token_entity_pairs_lst\n",
    "    w_str = \"\"\n",
    "    for pair in pairs:\n",
    "        w_str += f\"{pair[0]}\\t{pair[1]}\\n\"\n",
    "    w_str += \"\\n\"\n",
    "    all_w_str.append(w_str)\n",
    "\n",
    "for type_ in [\"train\", \"dev\", \"test\"]:\n",
    "    if type_ == 'train':\n",
    "        sub_pairs = all_w_str[:trn_idx]\n",
    "    elif type_ == 'dev':\n",
    "        sub_pairs = all_w_str[trn_idx:dev_idx]\n",
    "    else:\n",
    "        sub_pairs = all_w_str[dev_idx:]\n",
    "        \n",
    "    with open(f\"{ner_save_path}/ner_{type_}.txt\", \"w\") as f:\n",
    "        for _ in sub_pairs:\n",
    "            f.write(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0e4f7-8d9d-4e87-ba8d-de503d28f7e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DocTquery Dataset Generation (optional) ##\n",
    "- for an independent run, need all above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735b547-49da-4292-90a2-22ef1e10b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = \"9:1\"\n",
    "tot_pairs = len(all_dict['text_id'])\n",
    "d2q_trn_idx = int(tot_pairs * int(split_ratio.split(\":\")[0]) / 10)\n",
    "\n",
    "# We do not generate docTquery dataset with masked dataset (they are for clustering)    \n",
    "save_file_path_train_d2q = f\"data/synthetics/{dataset}_train_d2q.json\"\n",
    "save_file_path_dev_d2q = f\"data/synthetics/{dataset}_dev_d2q.json\"\n",
    "with open(save_file_path_train_d2q, 'w') as f1, open(save_file_path_dev_d2q, 'w') as f2:\n",
    "    ids_, texts_ = all_dict['text_id'], all_dict['d2q_text']\n",
    "    for idx_ in range(len(ids_)):\n",
    "        if idx_ < d2q_trn_idx:\n",
    "            f1.write(json.dumps({\"text_id\": ids_[idx_], \"text\": f\"Generate a question for the following passage: {texts_[idx_]}\"}) + '\\n')\n",
    "        else:\n",
    "            f2.write(json.dumps({\"text_id\": ids_[idx_], \"text\": f\"Generate a question for the following passage: {texts_[idx_]}\"}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4620be2-c929-45c9-8680-9c5881ce9dc7",
   "metadata": {},
   "source": [
    "## Mask Texts ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399e7b-c434-4746-bb33-37d1312fc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict({'docid': all_dict['docid'], 'text': all_dict['text']}).to_csv(save_file_path, sep=\"\\t\", index=False, header=False) \n",
    "pd.DataFrame.from_dict({'docid': all_dict['docid'], 'text': all_dict['text']}).to_csv(f\"data/synthetics/{dataset}_seen.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "sentences = [Sentence(text) for text in all_dict['text']]\n",
    "fine_tuned_tagger.predict(sentences, mini_batch_size=512, verbose=True)\n",
    "masked_texts = mask_text(sentences, verbalize=True)\n",
    "pd.DataFrame.from_dict({'docid': all_dict['docid'], 'text': masked_texts}).to_csv( f\"data/synthetics/{dataset}_masked.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "pd.DataFrame.from_dict({'docid': all_dict['docid'], 'text': masked_texts}).to_csv(f\"data/synthetics/{dataset}_seen_masked.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db7f17-21dd-4491-a2fe-700a0bcace2a",
   "metadata": {},
   "source": [
    "## DocID Assignment (K-means Clustering) ##\n",
    "- old_to_new_docid\n",
    "- masked_old_to_new_docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6422cc0-10a5-46a7-a24c-5a611dc7b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bert.sh 1 {dataset + \"_seen\"} && wait\n",
    "!./bert.sh 1 {dataset + \"_seen\" + \"_masked\"} && wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e153d-3cfc-48c8-875e-4752fc2ae1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./kmeans.sh {dataset + \"_seen\"} && wait\n",
    "!./kmeans.sh {dataset + \"_seen\" + \"_masked\"} && wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54488d53-48e3-44d4-a5f8-95b7232240a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'IDMapping_{dataset + \"_seen\"}_bert_512_k9_c20_seed_7.pkl', 'rb') as f:\n",
    "    kmeans_qdoc_dict = pickle.load(f)\n",
    "old_to_new_docid: dict = {k: ''.join([str(_) for _ in v]) for k, v in kmeans_qdoc_dict.items()}\n",
    "old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in kmeans_qdoc_dict.items()} \n",
    "\n",
    "with open(f'IDMapping_{dataset + \"_seen\" + \"_masked\"}_bert_512_k9_c20_seed_7.pkl', 'rb') as f:\n",
    "    masked_kmeans_qdoc_dict = pickle.load(f)\n",
    "# print(len(masked_kmeans_qdoc_dict))  # {0: [6, 9, 3, 1], 1: [8, 9, 7, 1], 2: [2, 1, 3, 1], 3: [8, 4, 6, 1], 4: [3, 4, 9, 1], 5: [2, 2, 3, 3, 1], 6: [2, 6, 8, 8, 1], 7: [8, 9, 2, 1], 8: [2, 3, 4, 1], 9: [8, 6, 9, 1], 10: [7, 6, 2, 1],\n",
    "masked_old_to_new_docid: dict = {k: ''.join([str(_) for _ in v]) for k, v in masked_kmeans_qdoc_dict.items()}\n",
    "masked_old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in masked_kmeans_qdoc_dict.items()}  # ['4 6 7 5', '8 9 9 6', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d46c1-9435-491c-80e1-bb6e4c11db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for `Query Generation` task (generation.yaml)\n",
    "df_ = pd.DataFrame.from_dict({'docid': list(old_to_new_docid_char.values()), 'text': all_dict['d2q_text']})\n",
    "df_['docid'] = df_['docid'].astype('str')\n",
    "df_.to_csv(f\"data/synthetics/{dataset}_seen_clustered.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "\n",
    "# Use for `Query Generation` task (generation.yaml)\n",
    "df_ = pd.DataFrame.from_dict({'docid': list(masked_old_to_new_docid_char.values()), 'text': all_dict['d2q_text']})\n",
    "df_['docid'] = df_['docid'].astype('str')\n",
    "df_.to_csv(f\"data/synthetics/{dataset}_seen_masked_clustered.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "\n",
    "df_ = pd.DataFrame.from_dict({'docid': list(masked_old_to_new_docid_char.values()), 'text': all_dict['text']})\n",
    "df_['docid'] = df_['docid'].astype('str')\n",
    "df_.to_csv(f\"data/synthetics/{dataset}_seen_masked_clustered_og.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "\n",
    "# df_ = pd.DataFrame.from_dict({'docid': list(old_to_new_docid_char.values()), 'text': all_dict['text_id']})\n",
    "# df_['docid'] = df_['docid'].astype('str')\n",
    "# df_.to_csv(f\"data/synthetics/{dataset}_seen_clustered_queries.tsv\", sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "\n",
    "df_ = pd.DataFrame.from_dict({'docid': list(masked_old_to_new_docid_char.values()), 'text': all_dict['text_id']})\n",
    "df_['docid'] = df_['docid'].astype('str')\n",
    "df_.to_csv(f\"data/synthetics/{dataset}_seen_masked_clustered_queries.tsv\", sep=\"\\t\", index=False, header=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd95b42d-b523-4195-a23d-c3c5b135c3de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Query-Passage Augmentation (optional) - deprecated ##\n",
    "- for an independent run, need all above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061478a9-536c-4b60-8d5d-0969053e7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, itertools, numpy as np, pandas as pd\n",
    "# from transformers import (AutoTokenizer,\n",
    "#                           AutoModelForSequenceClassification)\n",
    "# from flair.models import SequenceTagger\n",
    "# from flair.data   import Sentence\n",
    "# from tqdm.auto    import tqdm\n",
    "\n",
    "# DEVICE = \"cuda:0\"\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "# # 0.  Models & tokenizers\n",
    "# # -------------------------------------------------------------------\n",
    "# rer_tok  = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\", use_fast=True)\n",
    "# rer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#               \"BAAI/bge-reranker-v2-m3\",\n",
    "#               output_attentions=True).to(DEVICE).eval()\n",
    "\n",
    "# # Generate Swapped Entity Query Expansion\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "# # 1. Helper: run Flair NER and return entity list w/ char & token spans\n",
    "# # -------------------------------------------------------------------\n",
    "# def flair_entities(text):\n",
    "#     sent = Sentence(text) # , use_tokenizer=tokenizer)        # stable offsets\n",
    "#     fine_tuned_tagger.predict(sent)\n",
    "#     ents = []\n",
    "#     for ent in sent.get_spans(\"ner\"):\n",
    "#         ents.append({\"type\": ent.tag,\n",
    "#                      \"char_span\": (ent.start_position, ent.end_position)})\n",
    "#     return ents\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "# # 2. Helper: map char span -> token indices in BGE tokenizer\n",
    "# # -------------------------------------------------------------------\n",
    "# def char2tok_span(text, char_span, tok_offsets):\n",
    "#     \"\"\"Return (tok_start, tok_end) inclusive given a HuggingFace\n",
    "#        offsets_mapping list.\"\"\"\n",
    "#     c_start, c_end = char_span\n",
    "#     for i, (s, e) in enumerate(tok_offsets):\n",
    "#         if s <= c_start < e:\n",
    "#             tok_start = i\n",
    "#             break\n",
    "#     else:\n",
    "#         return None\n",
    "#     for j in range(tok_start, len(tok_offsets)):\n",
    "#         s, e = tok_offsets[j]\n",
    "#         if e >= c_end:\n",
    "#             tok_end = j\n",
    "#             break\n",
    "#     return tok_start, tok_end\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "# # 3.  Augment one (query, passage) pair\n",
    "# # -------------------------------------------------------------------\n",
    "# def augment_pair(id_, query, passage,\n",
    "#                  threshold=0.005,   # min avgâ€‘attention to accept swap\n",
    "#                  top_k=3):         # number of swaps per query entity\n",
    "#     # --- NER ---\n",
    "#     q_ents = flair_entities(query)\n",
    "#     p_ents = flair_entities(passage)\n",
    "\n",
    "#     # --- reranker   CLS  q  SEP  p  SEP\n",
    "#     enc = rer_tok(query, passage,\n",
    "#                   return_offsets_mapping=True,\n",
    "#                   max_length=512,\n",
    "#                   truncation=True,\n",
    "#                   return_tensors=\"pt\").to(DEVICE)\n",
    "#     offsets = enc.pop(\"offset_mapping\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         out  = rer_model(**enc, output_attentions=True, return_dict=True)\n",
    "\n",
    "#     # lastâ€‘layer attn avg over heads  â†’  [L, L]\n",
    "#     attn = out.attentions[-1].mean(dim=1).squeeze(0)    # fp16\n",
    "\n",
    "#     # split indices\n",
    "#     sep = (enc[\"input_ids\"][0] == rer_tok.sep_token_id).nonzero(as_tuple=True)[0][0]\n",
    "#     q_shift = 1                # skip CLS\n",
    "#     p_shift = sep + 1          # skip CLS + query + SEP\n",
    "\n",
    "#     offsets = offsets[0].tolist()  \n",
    "#     # print(offsets) \n",
    "#     # offsets = enc[\"offset_mapping\"][0].tolist()          # list[(s,e)]\n",
    "#     swaps = []\n",
    "\n",
    "#     # --- for every entity in query ---\n",
    "#     for q_ent in q_ents:\n",
    "#         if q_ent[\"type\"] == \"TIM\" or q_ent[\"type\"] == \"DAT\":\n",
    "#             continue\n",
    "#         q_tok_span = char2tok_span(query, q_ent[\"char_span\"], offsets[q_shift:sep])\n",
    "#         if not q_tok_span: continue\n",
    "#         q_t0, q_t1 = (q_tok_span[0] + q_shift, q_tok_span[1] + q_shift)\n",
    "\n",
    "#         # aggregate queryâ€‘entity attention vector\n",
    "#         q_vec = attn[q_t0:q_t1+1].mean(dim=0)            # [L]\n",
    "\n",
    "#         best = []\n",
    "#         for p_ent in p_ents:\n",
    "#             if p_ent[\"type\"] != q_ent[\"type\"]: continue\n",
    "#             p_tok_span = char2tok_span(passage, p_ent[\"char_span\"],\n",
    "#                                        offsets[p_shift:-1])  # exclude last SEP\n",
    "#             if not p_tok_span: continue\n",
    "#             p_t0, p_t1 = (p_tok_span[0] + p_shift, p_tok_span[1] + p_shift)\n",
    "\n",
    "#             score = q_vec[p_t0:p_t1+1].mean().item()\n",
    "#             best.append((score, p_ent))\n",
    "\n",
    "#         best = [b for b in sorted(best, key=lambda x: -x[0]) if b[0] >= threshold][:top_k]\n",
    "\n",
    "#         # --- create swapped texts ---\n",
    "#         for score, p_ent in best:\n",
    "#             q_new = (query[: q_ent[\"char_span\"][0]] +\n",
    "#                      passage[p_ent[\"char_span\"][0]: p_ent[\"char_span\"][1]] +\n",
    "#                      query[q_ent[\"char_span\"][1]:])\n",
    "\n",
    "#             p_new = (passage[: p_ent[\"char_span\"][0]] +\n",
    "#                      query[q_ent[\"char_span\"][0]: q_ent[\"char_span\"][1]] +\n",
    "#                      passage[p_ent[\"char_span\"][1]:])\n",
    "\n",
    "#             swaps.append({\"docid\": id_,\n",
    "#                           \"q_swapped\": q_new,\n",
    "#                           \"p_swapped\": p_new,\n",
    "#                           \"type\": q_ent[\"type\"],\n",
    "#                           \"score\": score})\n",
    "\n",
    "#     return swaps         # list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689405a-a471-452b-a234-08efe33dab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------------------------------------------\n",
    "# # 4.  Run over the whole dataset [THIS VERSION IS FOR ONLY MASKED DOCIDs]\n",
    "# # -------------------------------------------------------------------\n",
    "# pids_, ptexts_, _ = read_id_text_from_file(f\"data/synthetics/{dataset}_seen_masked_clustered_og.tsv\")\n",
    "# qids_, qtexts_, _ = read_id_text_from_file(f\"data/synthetics/{dataset}_seen_masked_clustered_queries.tsv\")\n",
    "# assert pids_ == qids_, f\"invalid id collisions\"\n",
    "\n",
    "# all_aug = []\n",
    "# all_pairs = {'docid': pids_, 'queries': qtexts_, 'passages': ptexts_}\n",
    "# for id_, q, p in tqdm(zip(all_pairs['docid'], all_pairs[\"queries\"], all_pairs[\"passages\"]),\n",
    "#                  total=len(all_pairs), desc=\"Augment\"):\n",
    "#     all_aug.extend(augment_pair(id_, q, p))\n",
    "# aug_df = pd.DataFrame(all_aug)\n",
    "\n",
    "# sentences = [Sentence(text) for text in aug_df['q_swapped']]\n",
    "# fine_tuned_tagger.predict(sentences, mini_batch_size=512, verbose=True)\n",
    "# aug_df['q_swapped_mask'] = mask_text(sentences, verbalize=False)\n",
    "\n",
    "# sentences = [Sentence(text) for text in aug_df['p_swapped']]\n",
    "# fine_tuned_tagger.predict(sentences, mini_batch_size=512, verbose=True)\n",
    "# aug_df['p_swapped_mask'] = mask_text(sentences, verbalize=False)\n",
    "\n",
    "# print(\"generated\", len(aug_df), \"new swapped pairs\")a\n",
    "\n",
    "# aug_df['docid'] = aug_df['docid'].astype('str')\n",
    "# with open(f\"data/synthetics/{dataset}_seen_masked_swap_augmented.json\", \"w\") as f:\n",
    "#     for idx_, row_ in aug_df.iterrows():\n",
    "#         f.write(json.dumps({\"text_id\": row_[\"docid\"], \"text\": row_[\"q_swapped_mask\"]}) + '\\n')\n",
    "#         f.write(json.dumps({\"text_id\": row_[\"docid\"], \"text\": row_[\"p_swapped_mask\"]}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10f1f3-80dd-45a6-87ef-a9eab32e5401",
   "metadata": {},
   "source": [
    "## Query-Passage Augmentation (optional) ##\n",
    "- for an independent run, need all above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d1a20-057b-4639-9565-062c054c0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# helpers -----------------------------------------------------\n",
    "# ------------------------------------------------------------\n",
    "import re, torch\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "START_TAG   = {\"<PER>\", \"<LOC>\", \"<EVT>\"}\n",
    "END_TAG_OF  = {\"<PER>\": \"</PER>\", \"<LOC>\": \"</LOC>\", \"<EVT>\": \"</EVT>\"}\n",
    "\n",
    "TagSpan = Tuple[str, int, int]   # (tag_type, start_tok_idx, end_tok_idx)\n",
    "\n",
    "\n",
    "def _extract_entity_spans(tokens: List[str]) -> List[TagSpan]:\n",
    "    \"\"\"\n",
    "    tokens : list of decoded tokens (already split by tokenizer)\n",
    "    returns: list of (tag_type, start, end)  -- indices inclusive\n",
    "            where start / end are token-indices **inside the current segment**\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tok = tokens[i]\n",
    "        if tok in START_TAG:\n",
    "            tag_type   = tok\n",
    "            end_tag    = END_TAG_OF[tag_type]\n",
    "            j = i + 1\n",
    "            # find matching end tag\n",
    "            while j < len(tokens) and tokens[j] != end_tag:\n",
    "                j += 1\n",
    "            if j < len(tokens):      # found end\n",
    "                spans.append((tag_type, i + 1, j - 1))  # Special entity token exclusive indices\n",
    "                i = j + 1\n",
    "            else:\n",
    "                i += 1               # malformed span; skip\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans\n",
    "\n",
    "\n",
    "def _segment_tokens(tokens: List[int]) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"Split [CLS] query [SEP] passage [SEP] into 2 token-index sets.\"\"\"\n",
    "    sep_idx = tokens.index(rer_tok.sep_token_id)\n",
    "    query_tok_ids    = list(range(1, sep_idx))          # exclude [CLS],[SEP]\n",
    "    passage_tok_ids  = list(range(sep_idx + 1, len(tokens) - 1))\n",
    "    return query_tok_ids, passage_tok_ids\n",
    "\n",
    "\n",
    "def _attention_mass(att, src_ids, tgt_ids):\n",
    "    \"\"\"\n",
    "    att : (N, N) â€“ single attention matrix (already averaged over heads/layers)\n",
    "    src_ids, tgt_ids : lists of token indices belonging to src / tgt span\n",
    "    returns scalar attention mass from src->tgt\n",
    "    \"\"\"\n",
    "    src = torch.tensor(src_ids, device=att.device)\n",
    "    tgt = torch.tensor(tgt_ids, device=att.device)\n",
    "    return att[src][:, tgt].sum() #  / src.size(0) this really depends \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# new augment_pair -----------------------------------------------------------\n",
    "# ------------------------------------------------------------\n",
    "def augment_pair(docid: str,\n",
    "                 query_str: str,\n",
    "                 passage_str: str,\n",
    "                 thr: float = 5e-2, # 5e-3,     # <- tune empirically; raw mass now!\n",
    "                 top_layer: int = -1) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Produce â‰¤|ð”ˆ(q)| swapped pairs â€“ one for each query entity-span that\n",
    "    has at least one passage span of the *same type* with raw-mass â‰¥ thr.\n",
    "\n",
    "    Returns a list of (new_docid, new_query, new_passage).\n",
    "    If **no** span meets `thr`, returns [].\n",
    "    \"\"\"\n",
    "    # 1) tokenise\n",
    "    enc = rer_tok(query_str, passage_str,\n",
    "                  return_tensors=\"pt\",\n",
    "                  add_special_tokens=True).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        att = rer_model(**enc,\n",
    "                        output_attentions=True,\n",
    "                        return_dict=True\n",
    "                       ).attentions[top_layer].mean(dim=1)[0]  # (N,N)\n",
    "\n",
    "    tok_ids = enc[\"input_ids\"][0].tolist()\n",
    "    dec_tok = rer_tok.convert_ids_to_tokens(tok_ids)\n",
    "    # print(dec_tok)\n",
    "\n",
    "    q_ids, p_ids = _segment_tokens(tok_ids)\n",
    "    q_spans = _extract_entity_spans([dec_tok[i] for i in q_ids])\n",
    "    p_spans = _extract_entity_spans([dec_tok[i] for i in p_ids])\n",
    "\n",
    "    # print(q_spans, p_spans)\n",
    "\n",
    "    # map to absolute indices\n",
    "    q_spans = [(t, q_ids[s], q_ids[e]) for (t, s, e) in q_spans]\n",
    "    p_spans = [(t, p_ids[s], p_ids[e]) for (t, s, e) in p_spans]\n",
    "\n",
    "    # print(dec_tok)\n",
    "    # print(query_str, passage_str, q_spans, p_spans)\n",
    "    \n",
    "    if not q_spans or not p_spans:\n",
    "        return []                       # nothing to swap\n",
    "\n",
    "    # ------- pick best passage span for every query span -------------------\n",
    "    candidates : Dict[int, Tuple[float, TagSpan]] = {}\n",
    "    for idx_q, (t_q, s_q, e_q) in enumerate(q_spans):\n",
    "        best = (0.0, None)              # (mass, (t,s,e))\n",
    "        \n",
    "        for t_p, s_p, e_p in p_spans:\n",
    "            if t_q != t_p:              # require same entity type\n",
    "                continue\n",
    "            mass = _attention_mass(att,\n",
    "                                   list(range(s_q, e_q + 1)),\n",
    "                                   list(range(s_p, e_p + 1))).item()\n",
    "            # print(mass)\n",
    "            if mass > best[0]:\n",
    "                best = (mass, (t_p, s_p, e_p))\n",
    "\n",
    "        if best[0] >= thr:              # keep only if above threshold\n",
    "            candidates[idx_q] = best\n",
    "\n",
    "    if not candidates:\n",
    "        return []                       # no span passed the threshold\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    aug_pairs = []\n",
    "    for k, (mass, (t_p, s_p, e_p)) in candidates.items():\n",
    "        # copy original tokens each time so swaps don't interfere\n",
    "        toks = dec_tok.copy()\n",
    "        t_q, s_q, e_q = q_spans[k]\n",
    "\n",
    "        span_q = toks[s_q:e_q + 1]  \n",
    "        span_p = toks[s_p:e_p + 1]\n",
    "\n",
    "        toks = toks[:s_q] + span_p + toks[e_q + 1:s_p] + span_q + toks[e_p + 1:]\n",
    "\n",
    "        sep = toks.index(rer_tok.sep_token)\n",
    "        new_query = rer_tok.convert_tokens_to_string(toks[1:sep])\n",
    "        new_pass  = rer_tok.convert_tokens_to_string(toks[sep + 2:-1])  # skip <SEP> </S>\n",
    "\n",
    "        aug_pairs.append({'docid': docid, 'q_text': new_query, 'p_text': new_pass})\n",
    "\n",
    "    return aug_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c6140-edf2-4a7c-8593-5a4d49a927b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_, ptexts_, _ = read_id_text_from_file(f\"data/synthetics/syn_50k_seen_masked_clustered_passages_masked_train\", is_json=True)\n",
    "qids_, qtexts_, _ = read_id_text_from_file(f\"data/synthetics/syn_50k_seen_masked_clustered_queries_train.json\", is_json=True)\n",
    "assert pids_ == qids_, f\"invalid id collisions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1ab48-643a-4440-84c3-b967fab682b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools, numpy as np, pandas as pd\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification)\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data   import Sentence\n",
    "from tqdm.auto    import tqdm\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0.  Models & tokenizers\n",
    "# -------------------------------------------------------------------\n",
    "rer_tok  = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\", use_fast=True)\n",
    "rer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "              \"BAAI/bge-reranker-v2-m3\",\n",
    "              output_attentions=True).to(DEVICE).eval()\n",
    "\n",
    "entity_tokens = [\"<PER>\", \"</PER>\", \"<LOC>\", \"</LOC>\", \"<TIM>\", \"</TIM>\", \"<EVT>\", \"</EVT>\"]\n",
    "rer_tok.add_tokens(entity_tokens)\n",
    "rer_model.resize_token_embeddings(len(rer_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cf41a-1d68-47bb-9983-14e2032ab866",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20000000\n",
    "all_pairs = {'docid': pids_[:num_samples], 'queries': qtexts_[:num_samples], 'passages': ptexts_[:num_samples]}\n",
    "all_augmented = []\n",
    "for docid, q, p in tqdm(zip(all_pairs[\"docid\"],\n",
    "                            all_pairs[\"queries\"],\n",
    "                            all_pairs[\"passages\"]),\n",
    "                        total=len(all_pairs),\n",
    "                        desc=\"Augment\"):\n",
    "    all_augmented.extend(augment_pair(docid, q, p))\n",
    "\n",
    "aug_df = pd.DataFrame(all_augmented)\n",
    "\n",
    "print(\"generated\", len(aug_df), \"new swapped pairs\")\n",
    "\n",
    "aug_df['docid'] = aug_df['docid'].astype('str')\n",
    "with open(f\"data/synthetics/{dataset}_seen_masked_swap_augmented.json\", \"w\") as f:\n",
    "    for idx_, row_ in aug_df.iterrows():\n",
    "        f.write(json.dumps({\"text_id\": row_[\"docid\"], \"text\": row_[\"q_text\"], \"is_tp\": 0}) + '\\n')\n",
    "        f.write(json.dumps({\"text_id\": row_[\"docid\"], \"text\": row_[\"p_text\"], \"is_tp\": 0}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc713aea-384e-4e16-b745-5ba764d32087",
   "metadata": {},
   "source": [
    "## PostID Mask Texts ##\n",
    "- after building docids, use them to adjust other files' ids accordingly\n",
    "- for an independent run, it requires run upto 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041be9e-9741-4969-b658-de2ae92b82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docid with masking + masked queries [Train Set]\n",
    "with open(f\"data/synthetics/{dataset}_seen_masked_clustered_queries_train.json\", 'w') as f:\n",
    "    ids_, texts_ = list(masked_old_to_new_docid_char.values()), all_dict['text_id']\n",
    "    \n",
    "    texts_ = [Sentence(text) for text in texts_]\n",
    "    fine_tuned_tagger.predict(texts_, mini_batch_size=512, verbose=True)\n",
    "    texts_ = mask_text(texts_)\n",
    "        \n",
    "    for idx_ in range(len(ids_)):\n",
    "        f.write(json.dumps({\"text_id\": ids_[idx_], \"text\": f\"{texts_[idx_]}\"}) + '\\n')\n",
    "\n",
    "\n",
    "with open(f\"data/synthetics/{dataset}_seen_clustered_queries_train.json\", 'w') as f:\n",
    "    ids_, texts_ = list(old_to_new_docid_char.values()), all_dict['text_id']\n",
    "    for idx_ in range(len(ids_)):\n",
    "        f.write(json.dumps({\"text_id\": ids_[idx_], \"text\": f\"{texts_[idx_]}\"}) + '\\n')\n",
    "\n",
    "\n",
    "# Get passage contents for training \n",
    "with open(f\"data/synthetics/{dataset}_seen_masked_clustered_og.tsv\", \"r\") as f:\n",
    "    ids_, passages_ = [], []\n",
    "    for data in f:\n",
    "        id_, passage_ = data.split(\"\\t\")\n",
    "        ids_.append(id_)\n",
    "        passages_.append(passage_)\n",
    "sentences = [Sentence(text) for text in passages_]\n",
    "fine_tuned_tagger.predict(sentences, mini_batch_size=512, verbose=True)\n",
    "with open(f\"data/synthetics/{dataset}_seen_masked_clustered_passages_masked_train\", \"w\") as f:\n",
    "    for id_, masked_passage_ in zip(ids_, mask_text(sentences)):\n",
    "        f.write(json.dumps({\"text_id\": id_, \"text\": masked_passage_}) + '\\n')\n",
    "\n",
    "\n",
    "# with open(f\"data/synthetics/{dataset}_seen_clustered.tsv\", \"r\") as f:\n",
    "#     ids_, passages_ = [], []\n",
    "#     for data in f:\n",
    "#         id_, passage_ = data.split(\"\\t\")\n",
    "#         ids_.append(id_)\n",
    "#         passages_.append(passage_)\n",
    "ids_, passages_ = list(old_to_new_docid_char.values()), all_dict['text']\n",
    "with open(f\"data/synthetics/{dataset}_seen_clustered_passages_train\", \"w\") as f:\n",
    "    for id_, passage_ in zip(ids_, passages_):\n",
    "        f.write(json.dumps({\"text_id\": id_, \"text\": passage_}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a548771-a2c9-43d5-903a-6411cff5b3b4",
   "metadata": {},
   "source": [
    "## Post-QG Process ##\n",
    "- You have to run **GENERATION** twice before running this. Files required are:\n",
    "    - [1] `{dataset}_seen_clustered.tsv`,\n",
    "    - [2] `{dataset}_seen_masked_clustered.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fc40f-f38c-492e-bfa5-c5d4126678bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d2q_file_path = f\"data/synthetics/{dataset}_seen_clustered.tsv.q15.docTquery\"\n",
    "train_data = datasets.load_dataset(\n",
    "    'json',\n",
    "    data_files=d2q_file_path,\n",
    "    ignore_verifications=False\n",
    ")['train']\n",
    "print(f\"Finished loading dataset\")\n",
    "\n",
    "masked_d2q_file_path = f\"data/synthetics/{dataset}_seen_masked_clustered.tsv.q15.docTquery\"\n",
    "masked_train_data = datasets.load_dataset(\n",
    "    'json',\n",
    "    data_files=masked_d2q_file_path,\n",
    ")['train']\n",
    "masked_text_ids = masked_train_data['text_id']\n",
    "print(f\"Finished loading dataset\")\n",
    "\n",
    "sentences = [Sentence(text) for text in train_data['text']]\n",
    "fine_tuned_tagger.predict(sentences, mini_batch_size=768, verbose=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "# After QG, create the queries with annotation version (text_id = docid, text = text)\n",
    "new_docTquery = {'text_id': train_data['text_id'], 'text': mask_text(sentences), 'masked_text_id': masked_text_ids}\n",
    "with open(d2q_file_path + \".masked\", 'w') as f1, open(masked_d2q_file_path + \".masked\", 'w') as f2:\n",
    "    for idx_ in tqdm(range(len(new_docTquery['text_id']))):\n",
    "        f1.write(json.dumps({\"text_id\": new_docTquery['text_id'][idx_], \"text\": new_docTquery['text'][idx_]}) + '\\n')\n",
    "        f2.write(json.dumps({\"text_id\": new_docTquery['masked_text_id'][idx_], \"text\": new_docTquery['text'][idx_]}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cf265-4342-40c1-bdc5-e8cb696fcfbe",
   "metadata": {},
   "source": [
    "## Final Train Files Concatenation ##\n",
    "\n",
    "+ `{dataset}_seen_masked_clustered_queries_train.json` (train query annotated)                     \n",
    "+ `{dataset}_seen_masked_clustered_passages_masked_train`  (train passages annotated)            \n",
    "+ `{dataset}_seen_masked_clustered.tsv.q10.docTquery.masked` (generated queries annotated)         \n",
    "\n",
    "\n",
    "For no Annotation train file:\n",
    "+ `{dataset}_seen_clustered_queries_train.json` (train query)                                 \n",
    "+ `{dataset}_seen_clustered_passages_train`  (train passages)                                 \n",
    "+ `{dataset}_seen_clustered.tsv.q10.docTquery` (generated queries)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb14b72-5755-4bdb-8f0f-7d9903f48990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all_files(files, out):\n",
    "    all_ids, all_texts = [], []\n",
    "    tp_indices = None\n",
    "    for file in files:\n",
    "        ids_, texts_, _ = read_id_text_from_file(file, is_json=True)\n",
    "        if 'passages' in file:\n",
    "            tp_indices = [len(all_ids), len(all_ids) + len(ids_)]\n",
    "        all_ids.extend(ids_)\n",
    "        all_texts.extend(texts_)\n",
    "    print(len(all_ids))\n",
    "\n",
    "    with open(out, \"w\") as f:\n",
    "        for idx_ in tqdm(range(len(all_ids))):\n",
    "            is_tp = 1 if tp_indices[0] <= idx_ < tp_indices[1] else 0\n",
    "            f.write(json.dumps({\"text_id\": all_ids[idx_], \"text\": all_texts[idx_].replace('\\n', ''), \"is_tp\": is_tp}) + '\\n')\n",
    "    print(f\"{out} generated successfully\")\n",
    "    \n",
    "train_files = [\n",
    "    f\"data/synthetics/{dataset}_seen_masked_clustered_queries_train.json\",\n",
    "    f\"data/synthetics/{dataset}_seen_masked_clustered_passages_masked_train\",\n",
    "    f\"data/synthetics/{dataset}_seen_masked_clustered.tsv.q15.docTquery.masked\"\n",
    "]\n",
    "\n",
    "train_files_noA = [\n",
    "    f\"data/synthetics/{dataset}_seen_clustered_queries_train.json\",\n",
    "    f\"data/synthetics/{dataset}_seen_clustered_passages_train\",\n",
    "    f\"data/synthetics/{dataset}_seen_clustered.tsv.q15.docTquery\"\n",
    "]\n",
    "\n",
    "concat_all_files(files=train_files, out=f'data/synthetics/{dataset}_train_pearl')\n",
    "concat_all_files(files=train_files_noA, out=f'data/synthetics/{dataset}_train_noA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf66a7-66a9-4718-b44d-70b7b235671c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Re-assign DocIDs with New DocIDs (optional)##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125db238-e138-4887-b84c-8bc9632a3dae",
   "metadata": {},
   "source": [
    "### Previous Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e738b-a4c5-49a9-99a6-263a83e95302",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'IDMapping_{dataset + \"_seen\"}_bert_512_k9_c20_seed_7.pkl', 'rb') as f:\n",
    "    kmeans_qdoc_dict = pickle.load(f)\n",
    "old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in kmeans_qdoc_dict.items()} \n",
    "\n",
    "with open(f'IDMapping_{dataset + \"_seen\" + \"_masked\"}_bert_512_k9_c20_seed_7.pkl', 'rb') as f:\n",
    "    masked_kmeans_qdoc_dict = pickle.load(f)\n",
    "masked_old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in masked_kmeans_qdoc_dict.items()}  # ['4 6 7 5', '8 9 9 6', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00d505-8aba-4cd3-a871-f639501a9231",
   "metadata": {},
   "source": [
    "### New Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f68427-d879-4646-b0a8-271e60d303ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'IDMapping_{dataset + \"_seen\"}_roberta_512_k5_c20_seed_7.pkl', 'rb') as f:  # _roberta_512_k5_c20_seed_7.pkl, _bert_512_k20_c20_seed_7.pkl\n",
    "    kmeans_qdoc_dict = pickle.load(f)\n",
    "updated_old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in kmeans_qdoc_dict.items()} \n",
    "\n",
    "with open(f'IDMapping_{dataset + \"_seen\" + \"_masked\"}_roberta_512_k5_c20_seed_7.pkl', 'rb') as f:  # _roberta_512_k5_c20_seed_7.pkl, _bert_512_k20_c20_seed_7.pkl\n",
    "    masked_kmeans_qdoc_dict = pickle.load(f)\n",
    "updated_masked_old_to_new_docid_char: dict = {k: ' '.join([str(_) for _ in v]) for k, v in masked_kmeans_qdoc_dict.items()}  # ['4 6 7 5', '8 9 9 6', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a893e-8f54-4044-a16e-5a7eb54339e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_to_updated_mapping = {old_to_new_docid_char[idx_]: updated_old_to_new_docid_char[idx_] for idx_ in range(len(updated_old_to_new_docid_char))}\n",
    "masked_old_to_updated_mapping = {masked_old_to_new_docid_char[idx_]: updated_masked_old_to_new_docid_char[idx_] for idx_ in range(len(updated_old_to_new_docid_char))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7d70b-a054-4911-9581-841e2dab647b",
   "metadata": {},
   "source": [
    "### Apply New Mapping\n",
    "- currently only support `masked` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e788ac0-4e1f-4bbc-ab47-70b6d164500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/synthetics/syn_50k_train_perl'\n",
    "qpa_file = 'data/synthetics/syn_50k_seen_masked_swap_augmented.json'\n",
    "files = [\n",
    "    train_file,  \n",
    "    qpa_file\n",
    "    ]\n",
    "for file in files:\n",
    "    ids, texts, tps = read_id_text_from_file(file, is_json=True, has_tp=True)\n",
    "    with open(file + \"_updated_rb_k5_c20\", 'w') as f:\n",
    "        for idx_ in range(len(ids)):\n",
    "            f.write(json.dumps({\"text_id\": masked_old_to_updated_mapping[ids[idx_]], \"text\": texts[idx_], \"is_tp\": tps[idx_]}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00a1d5-f844-4210-acdc-1909c88e4a63",
   "metadata": {},
   "source": [
    "# Test Dataset #\n",
    "- for an independent run, needs:\n",
    "    - raw .json files\n",
    "    - all *results* from run upto ~ Train 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d88fd8-9732-4022-bdef-bc23c2f54659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "use_annotation = True\n",
    "#################################\n",
    "\n",
    "\n",
    "unseen_raw_path = f\"data/synthetics/{dataset}_unseen/raw\"\n",
    "for unseen_raw_file in os.listdir(unseen_raw_path):\n",
    "    if os.path.isdir(os.path.join(unseen_raw_path, unseen_raw_file)):\n",
    "            continue\n",
    "    print(f\"Start preprocessing unseen {unseen_raw_file} ...\")\n",
    "\n",
    "    # Get unseen instances' cluster ids with existing k-means cluster \n",
    "    with open(os.path.join(unseen_raw_path, unseen_raw_file), 'r') as f:\n",
    "        unseen_dict_ = parse_json(json.load(f)) # text (simplified-passage), d2q_text (full-passage), text_id (q), q_ct, p_ct\n",
    "\n",
    "        # Get masked (verbalized) passages\n",
    "        passages = [Sentence(passage) for passage in unseen_dict_['text']]\n",
    "        fine_tuned_tagger.predict(passages, mini_batch_size=512, verbose=True)\n",
    "        unseen_dict_['masked_text'] = mask_text(passages, verbalize=True) if use_annotation else unseen_dict_['text']\n",
    "        unseen_cluster_ids, unseen_cluster_char_ids = get_exisiting_cluster_ids(test_masked_texts=unseen_dict_['masked_text'], use_annotation=use_annotation)\n",
    "\n",
    "    preprocessed_path = f\"data/synthetics/{dataset}_unseen/prep/{unseen_raw_file[:-5]}\" if use_annotation else f\"data/synthetics/{dataset}_unseen/prep_noA/{unseen_raw_file[:-5]}\"\n",
    "    Path(preprocessed_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create a queries dev set (**valid_file**)\n",
    "    with open(os.path.join(preprocessed_path, \"queries_dev.json\"), 'w') as f:\n",
    "        queries = [Sentence(_) for _ in unseen_dict_['text_id']]\n",
    "        fine_tuned_tagger.predict(queries, mini_batch_size=512, verbose=True)\n",
    "        masked_queries = mask_text(queries, verbalize=False) if use_annotation else unseen_dict_['text_id']\n",
    "\n",
    "        # Save the masked queries with cluster ids\n",
    "        for idx_ in range(len(unseen_cluster_char_ids)):\n",
    "            f.write(json.dumps({\"text_id\": unseen_cluster_char_ids[idx_], \"text\": f\"Question: {masked_queries[idx_]}\"}) + '\\n')\n",
    "\n",
    "    \n",
    "    # Create a original passage file (**passage_file**)\n",
    "    df_ = pd.DataFrame.from_dict({'docid': unseen_cluster_char_ids, 'text': unseen_dict_['text']})\n",
    "    df_['docid'] = df_['docid'].astype('str')\n",
    "    df_.to_csv(os.path.join(preprocessed_path, \"passages_og_dev.tsv\"), sep=\"\\t\", index=False, header=False) \n",
    "\n",
    "    \n",
    "    # Create a original query file (**query_file**)\n",
    "    df_ = pd.DataFrame.from_dict({'docid': unseen_cluster_char_ids, 'text': unseen_dict_['text_id']})\n",
    "    df_['docid'] = df_['docid'].astype('str')\n",
    "    df_.to_csv(os.path.join(preprocessed_path, \"queries_og_dev.tsv\"), sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    \n",
    "    # Create query and passage time mapping files (**passage_time_mapping**, **query_time_mapping**) \n",
    "    with open(os.path.join(preprocessed_path, \"queries_time_mapping\"), \"w\") as f1, open(os.path.join(preprocessed_path, \"passages_time_mapping\"), \"w\") as f2: \n",
    "        for id_, p_sent, q_sent, p_ct, q_ct in tqdm(zip(unseen_cluster_char_ids, passages, queries, unseen_dict_['p_ct'], unseen_dict_['q_ct'])):\n",
    "            p_dat_res, q_dat_res = extract_dat_from_sent(p_sent, datetime.fromisoformat(p_ct)), extract_dat_from_sent(q_sent, datetime.fromisoformat(q_ct))\n",
    "            f1.write(json.dumps({\"text_id\": id_, \"ct\": q_ct, \"t_s_time\": q_dat_res[0], \"t_e_time\": q_dat_res[1]}) + '\\n')\n",
    "            f2.write(json.dumps({\"text_id\": id_, \"ct\": p_ct, \"t_s_time\": p_dat_res[0], \"t_e_time\": p_dat_res[1]}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14336c9-95ed-4cee-9940-f32f77d2b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
